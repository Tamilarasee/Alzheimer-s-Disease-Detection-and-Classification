{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_hx8SJJR4-"
      },
      "source": [
        "# Assignment 2 - Logistic Regression & Naive Bayes\n",
        "\n",
        "Due by 11:59pm, March 23, 2023\n",
        "\n",
        "Name: Tamilarasee Sethuraj\n",
        "\n",
        "ID: A20553416"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyGrk2X_J_1h"
      },
      "source": [
        "## Theory Questions (Full points: 40, each question 5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8OnMEI0JXxA"
      },
      "source": [
        "**1. Explain the importance of setting up learning rate in the gradient descent based methods. For example, what would happen if the learning rate is too large/small?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv3iz5dBJX7Z"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Gradient descent is used to find the appropriate values of the parameters (theta) such that we get a minimum possible cost function in a machine learning algorithm. In order to bring the paramerters closer to the appropriate value, we update the parameters based on the product of the gradient descent value and a learning parameter. Whie gradient descent which is the slope of the cost curve helps to determine the direction (+ve/-ve) in which we should update our parameters, the learning rate determines how big/small (size) is the step in that direction determined by GD.\n",
        "\n",
        "Learning rate helps to know the scale at which the parameter gets varied at every update. If the learning parameter is too low, then we take very small steps towards the minimum cost and hence we reach the minimum value very slowly and we might not even reach the minimum if we have less number of epochs/iterations of update. So, it may need more time (slower convergence) and a large number of epochs to reach the minimum. We may also get stuck at the local minimum instead of reaching the global minimum because once we are in th eminimum, GD does not provide any direction and does not let us move further.\n",
        "\n",
        "If the learning parameter is too high, then we take very big steps(overshoot) that we might jump around the minimum cost but may not reach the minimum cost as we keep varying the parameters and the value updates on larger scale in the alternate directions given by GD and hence we may never reach the minimum value leading to divergence.\n",
        "\n",
        "It is always better to have an optimal value of learning rate which can be found by triail/error method or other dynamic techniques so that we reach(converge) the minimum cost at appropriate pace (possibly faster).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO8kErgCJX-P"
      },
      "source": [
        "**2. What is the stochastic gradient descent? Why do we need stochastic gradient descent?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEviYAL2JeWs"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "1. The Stochastic gradient descent is a method in which we update the parameter value after reviewing a randomly chosen single sample point at a time.\n",
        "2. Batch/vanilla gradient descent is a method which gets the feedback of all the training samples before it makes a direction decision to move. It reviews all the samples and then deicides the direction to move which is given to the parameter update rule for every step/update. This slows down the process of moving towards the minimum and take more time.\n",
        "3. In Stochastic gradient, as soon as we see a random training sample, the parameter is updated based on the feedback of this sample and then from the next updated step, a different random sample is used to direct the learning of the model. This randomness helps to explore the nature of the training samples and ensures that it doesnt get stuck at local minimum. \n",
        "4. Since we see one sample at a time, we may avoid overfitting the model thereby a new testing data could be efficiently predicted in this model.\n",
        "5. Although this approach of stochastic GD may result in random/wild oscillations of the direction, it would definetly progress towards the minimum cost since we choose samples differently each time and the progress is faster than the Batch GD.\n",
        "\n",
        "Thus stochastic GD is needed to converge faster (reach the minimum cost) by taking less time to decide the direction of the path especially for larger training sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vQD3sgwJYBS"
      },
      "source": [
        "**3. Explain the reasons to perform feature scaling.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-e37RdDJfmd"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Feature scaling is used to bring all the features to the similar scale range by using the  method of mean normalization/standardization.\n",
        "\n",
        "$$ùë•_{ùëñ}^ùëü = \\frac{(ùë•_{ùëñ}^ùëü ‚àí ùëö_{ùëñ})}{ùúé_{ùëñ}} $$\n",
        "\n",
        "where x is one of the feature of a given sample and ùëö and ùúé are the mean and standard deviation of that feature.\n",
        "\n",
        "We can also use the formula, $$X = \\frac{X ‚àí min(X)}{max(X) - min(X)} $$\n",
        "\n",
        "\n",
        "Feature scaling helps in making sure that every feature irrespective of their value ranges contributes equally to the progression of the model which provides better testing accuracy.\n",
        "\n",
        "Feature scaling helps in enhancing the speed of convergence. Say if there are 2 parameters , one of high value ranges and another of less range , then the loss function plots are skinny and lengthier which result in slowing down the gradient algorithm from reaching the minimum(center).If we could scale the parameters to have equal(apprx same) ranges/intervals, the function would be much more circular and the gradient can reach the min center in fewer steps. It also helps in better visualization and interpretation of the function. Also, when we use regularization in the model, it may not penalize the appropriate features without scaling them.\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOqhvEpuJYEX"
      },
      "source": [
        "**4. What is the probabilistic generative model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf5JjDFDJe6G"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Probabilistic generative model is one which implements the generative model approach where we model the joint distribution p(x,Ck) directly [which involves finding the p(x|C) and p(C)] and then normalize to obtain the posterior possibilities p(C|x). We then use the decision theory to determine class membership for each new input x through Bayes‚Äô theorem.\n",
        "\n",
        "Given an x, which of the 2 classes does it belong to \n",
        "$$ ùëÉ(ùê∂_1 | ùë•) =\\frac{ùëÉ(ùë•|ùê∂_1 )ùëÉ(ùê∂_1 )}{ùëÉ(ùë•|ùê∂_1 )ùëÉ(ùê∂_1 )+ùëÉ(ùë•|ùê∂_2 )ùëÉ(ùê∂_2 )}$$\n",
        "\n",
        "\n",
        "Generative Model ,\n",
        "\n",
        "$$ùëÉ(ùë•) = ùëÉ(ùë•|ùê∂_1 )ùëÉ(ùê∂_1 )+ùëÉ(ùë•|ùê∂_2 )ùëÉ(ùê∂_2 )$$\n",
        "\n",
        "A generative model describes how a dataset is generated, in terms of a probabilistic model.Thus,the probabilistic generative model provides a framework for understanding the underlying probability distributions of the data and enables us to make probabilistic predictions and generate new data points by sampling new data points that resemble the original dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8V802XDJjxe"
      },
      "source": [
        "**5. Explain how we perform maximum likelihood.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3PrgB7kJl99"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Probability of data is given by p(y|X; Œ∏) which is probability of y given x parametrized by theta. Likelihood function is given by L(Œ∏; X, y) in which the data is given/made fixed, while we see it as a function of parameter. Here, we try to find the values of the parameter for a given input and output.\n",
        "\n",
        "The principal of maximum likelihood says that we should choose parameters(Œ∏) so as to make the data as high probability as possible. I.e., we should choose Œ∏ to maximize L(Œ∏).Thus, maximum likelihood estimation involves finding the parameter values that maximize the likelihood of observing the given data, and it is commonly performed by maximizing the log-likelihood function for simplicity and ease of computation.\n",
        "\n",
        "Instead of maximizing L(Œ∏), we can also maximize any strictly increasing function of L(Œ∏). In particular, the derivations will be a bit simpler if we\n",
        "instead maximize the log likelihood l(Œ∏). Therefore, maximum likelihood estimations are done by taking the logarithm of the likelihood and equating the derivative to be zero. Using this equation, we find the parameters of the equation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kUcM98LJj9N"
      },
      "source": [
        "**6. Explain the reasons about using cross entropy loss in logistic regression.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuECRF2PJk81"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "In logistic regression, we use probablistic approach of classifying the ouput. If there are multiple classes, then we find the probablistic distribution of each class being the ouput for a given set of features. Now, when we compare this distribution with the original output, the comparision is between two (bernoulli) distributions and we wanted to minimize the gap between then. \n",
        "\n",
        "Comparsion between two distribution represents the cross entropy and we would like to reduce the distance between the distribution and hence we use the cross entropy loss minimization in logistic regression.\n",
        "\n",
        "Another reason why we do not use Mean square error as in linear regression is because the derivative of the MSE becomes zero as the ouput involves classification between values 0 and 1. \n",
        "\n",
        "Cross entropy formula is esentially the logarithm of the maximum likelihood function. If we use the maximimum likelihood funtion directly, we would have to focus on maximizing the function and  gradient ascent method instead of descent. In our upcoming model, we use the approach of taking negative of the log (maximum likelihood) and find parameters to minimize it.\n",
        "\n",
        "Maximim likelihood , $$L(\\theta) = ùë¶ùëì(ùë• )+(1‚àíùë¶ )(1‚àíùëì(ùë• ))$$\n",
        "\n",
        "Negative log likelihood, $$l(\\theta) = - [ùë¶.log (ùëì(ùë• ))+(1‚àíùë¶ ). log((1‚àíùëì(ùë• )))]$$\n",
        "\n",
        "where,\n",
        "$$ f(x) = \\sigma(z)=\\frac{1}{1+e^{-z}}$$\n",
        "$$z =\\sum \\limits _{i=1} ^{m} w_{i}x_{i} + bias $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8bCRVRaJkBP"
      },
      "source": [
        "**7. Explain the differences between discriminative and generative model.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEdAdRiu1ksO"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Generative model:\n",
        "\n",
        "1. Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space.\n",
        "2. In generative model, since use probability distribution, less training data is needed and it is more robust to the noise.\n",
        "3. Priors and class-dependent probabilities can be estimated from different sources.\n",
        "4. Generative model has more application besides classification as well.\n",
        "\n",
        "\n",
        "\n",
        "Discriminative model:\n",
        "\n",
        "1. First solve the inference problem of determining the posterior class probabilities p(Ck|x), and then subsequently use decision theory to assign each new x to one of the classes. Approaches that model the posterior probabilities directly are called discriminative models\n",
        "2. Discriminative model is more often used only for classification problem as it discriminates between classes.\n",
        "3. In discriminative models, to find the probability, the model directly assume some functional form for P(Y|X) and then estimate the parameters of P(Y|X) with the help of the training data.\n",
        "4. Discriminative Model uses a decision boundary between classes\n",
        "\n",
        "\n",
        "\n",
        "For larger datasets,discrminative model would help as it is acceptable to make weaker assumption and let the algorithm decide further towards the output, as we have a lot of data.For smaller datasets, Generative would be the computationally efficient option but the we have to make the assumption correctly whether the distribution is gaussian or Poisson, because the strong wrong assumption would definitely perform poorly.\n",
        "\n",
        "Generative is computationally efficient because there is no iterative process involved, it just needs the means and covariance calculated.Generative model works well on datasets with missing data as well as it is generative than the discriminativeone.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcZpkIc01Rwe"
      },
      "source": [
        "**8. What is N-folds? Explain the reasons why we need N-folds.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-h519EbJmbz"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "N folds refers to the cross-Validation which is a technique for evaluating the performance of a given data model. We can run the technique on various models and pick the most appropriate model based on the performance comparison\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Reaarange the given dataset randomly\n",
        "2. Split the dataset into N equal folds(groups)\n",
        "3. Pick one fold as Test Set and consider the remaining N-1 folds as Training Set\n",
        "4. Pick a model to evaluate and fit it on the Training Set, validate on the Test Set and save the validation score and discard the model\n",
        "5. Repeat the steps 3 & 4 N times for the same model, but pick a different fold as Test set each time such that every sample had the chance to be on the Test set once and on the training set for N-1 times.\n",
        "6. The average of N validation results is the final score for a given model.\n",
        "7. Repeat the same for different models and compare the average scores of every model to identify the best model for the machine.\n",
        "\n",
        "Reasons:\n",
        "\n",
        "1. Performing N-fold cross validation helps to evaluate how our model performs as we use different sets/folds to train the model each time and a different fold for assessing the performance.\n",
        "2. This can be done across different models and we can use the average accuracy score of each model to evaluate which is the bets model for our dataset.\n",
        "3. Since the framing of folds involve randomness, it helps to get a more reliable/general and distributed sense of the performance and how it would perform on unseen data thereby avaoiding overfitting the model.\n",
        "4. Every sample is used for both training and validation at some point of time in the process of our performance evaluation thereby maximizing the utilization of the available data.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AiWTVDf7cZ2"
      },
      "source": [
        "## Programming Questions (Full points: 60, each question 30 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hUXdV3L376cA"
      },
      "outputs": [],
      "source": [
        "# Do not edit the codes in this cell\n",
        "# load required library\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "# load dataset\n",
        "x, y = load_wine(return_X_y=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iwaee_T-Urj"
      },
      "source": [
        "### - Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyrt2Ka7VFYK"
      },
      "source": [
        "0. train_test_split (Done in the below cell)\n",
        "\n",
        "We **randomly** split data into train and test set. The number of training data and testing data is 100 and 30, respectively. Do not modify the split data. You need to use training data to train your model and obtain an optimal solution. Finally, using your model with the optimal solution to predict the testing data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9M1DAu9XPY8E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 13) (100,) (30, 13) (30,)\n"
          ]
        }
      ],
      "source": [
        "# Do not edit the codes in this cell\n",
        "# We split train and test data for logistic regression function\n",
        "\n",
        "test_lists = [0, 4, 5, 7, 13, 15, 19, 27, 28, 30, 31, 34, 39, 47, 63, 74, 78, 83, 90, 92, 95, 97, 103, 113, 119, 122, 123, 125, 126, 127]\n",
        "\n",
        "x = x[:130, :]\n",
        "y = y[:130]\n",
        "\n",
        "# original dataset has 3 classes (0,1,2) only upto 130 is taken for assignment so as to colect only data of class 0 and 1 from the dataset, \n",
        "\n",
        "# training data: you can use it to training your model.\n",
        "train_x = np.array([sub_x for index, sub_x in enumerate(x) if index not in test_lists])\n",
        "train_y = np.array([sub_y for index, sub_y in enumerate(y) if index not in test_lists])\n",
        "\n",
        "# testing data: ONLY use it to measure your model, do NOT use during training.\n",
        "test_x = np.array([sub_x for index, sub_x in enumerate(x) if index in test_lists])\n",
        "test_y = np.array([sub_y for index, sub_y in enumerate(y) if index in test_lists])\n",
        "\n",
        "# print(len(train_x[1])) - 13 features\n",
        "# load_wine().feature_names\n",
        "# load_wine().DESCR\n",
        "print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LdQ5yubLJld"
      },
      "source": [
        "In the assignment 2, you have more freedom on your programming design. In this part, you are going to implement your own Logistic Regression function. You need to implement logistic regression with stochastic gradient descent from scratch. The required functions are listed below. You can add more functions as you need. **No library versions of logistic regression are allowed**. \n",
        "_________\n",
        "\n",
        "1. train_val_split\n",
        "\n",
        "**Randomly** split training data into train and val set. 80% of the training data will be the train set and 20% of the training data will be the val set.\n",
        "\n",
        "2. normalization (data preprocessing)\n",
        "\n",
        "You should normalize all data for each attribute firstly. \n",
        "\n",
        "3. sigmoid\n",
        "\n",
        "The core of logistic regression\n",
        "\n",
        "4. predict\n",
        "\n",
        "Predict an output value for a given x with a set of coefficients. \n",
        "\n",
        "5. accurate\n",
        "\n",
        "Calculate accuracy percentage of the predictions.\n",
        "\n",
        "6. coef_gd\n",
        "\n",
        "Estimate logistic regression coefficients using **vanilla gradient descent**. Using **the cross entropy loss**. Carefully choose learning rate and epochs values.\n",
        "\n",
        "7. draw_model\n",
        "\n",
        "a) Plot the both training loss and validation loss for each epochs.\n",
        "\n",
        "b) Plot the both training accuracy and validation accuracy for each epochs. \n",
        "\n",
        "8. predict the testing data\n",
        "\n",
        "Use your pre-trained model to predict the testing data. Print out your **testing accurate**. Is it good? If not, analyze the reason in short and modify your code to improve.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. train_val_split\n",
        "\n",
        "**Randomly** split training data into train and val set. 80% of the training data will be the train set and 20% of the training data will be the val set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6AHamaj3f5xY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(80, 13) (80, 1) (20, 13) (20, 1)\n"
          ]
        }
      ],
      "source": [
        "# data split without scikit library\n",
        "\n",
        "def train_val_split(train_x,train_y):\n",
        "    \n",
        "    # import random\n",
        "    # # fixing a random seed for better interpretation, can remove if needed\n",
        "    # random.seed(50)  \n",
        "    # num_cases = len(train_y)\n",
        "    # num_val_set = int(num_cases*0.2)\n",
        "    # val_set_indices = []    \n",
        "    # count = 1\n",
        "\n",
        "    # while(count<=num_val_set):\n",
        "    #     index = random.randint(0,num_cases-1)       \n",
        "    #     if index not in val_set_indices:\n",
        "    #         val_set_indices.append(index)            \n",
        "    #         count+=1\n",
        "    \n",
        "\n",
        "    # val_set_x = np.array([value for index, value in enumerate(train_x) if index in val_set_indices])\n",
        "    # val_set_y = np.array([value for index, value in enumerate(train_y) if index in val_set_indices])\n",
        "        \n",
        "    # train_set_x = np.array([value for index, value in enumerate(train_x) if index not in val_set_indices])\n",
        "    # train_set_y = np.array([value for index, value in enumerate(train_y) if index not in val_set_indices])\n",
        "        \n",
        "\n",
        "    # # print(num_cases,len(val_set_x),len(val_set_y),len(train_set_x),len(train_set_y))\n",
        "    # # print(val_set_indices)\n",
        "    \n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_set_x, val_set_x, train_set_y, val_set_y = train_test_split(train_x, train_y, test_size=0.2, random_state=1)\n",
        "    train_set_y = train_set_y.reshape(-1,1)\n",
        "    val_set_y = val_set_y.reshape(-1,1)\n",
        "\n",
        "    return train_set_x,train_set_y,val_set_x,val_set_y\n",
        "\n",
        "\n",
        "train_set_x,train_set_y,val_set_x,val_set_y = train_val_split(train_x,train_y)\n",
        "print(train_set_x.shape,train_set_y.shape,val_set_x.shape,val_set_y.shape)\n",
        "\n",
        "# using scikit library\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# train_set_x, val_set_x, train_set_y, val_set_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. normalization (data preprocessing)\n",
        "\n",
        "You should normalize all data for each attribute firstly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# biasing any feature vector\n",
        "\n",
        "def bias(feature_vector):\n",
        "    biased_feature_vector = np.concatenate((np.ones((feature_vector.shape[0],1)),feature_vector),axis=1)\n",
        "    return biased_feature_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature scaling per formula\n",
        "def normalize(biased_feature_vector):\n",
        "    \n",
        "    mean = np.mean(biased_feature_vector,axis=0)\n",
        "    std = np.std(biased_feature_vector,axis=0)\n",
        "    epsilon = 1e-8\n",
        "    normalized_feature_vector = ( biased_feature_vector - mean ) / (std +epsilon)\n",
        "    # we use epsilon to handle the runtime warning of zero deniminator\n",
        "    return normalized_feature_vector\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(80, 13) (80, 1) (20, 13) (20, 1) (80, 14) (80, 14)\n"
          ]
        }
      ],
      "source": [
        "# Split,bias and normalize using respective functions\n",
        "\n",
        "train_set_x,train_set_y,val_set_x,val_set_y = train_val_split(train_x,train_y)\n",
        "train_set_x_biased = bias(train_set_x)\n",
        "train_set_x_normalized = normalize(train_set_x_biased)\n",
        "print(train_set_x.shape,train_set_y.shape,val_set_x.shape,val_set_y.shape,train_set_x_biased.shape,train_set_x_normalized.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. sigmoid\n",
        "\n",
        "The core of logistic regression\n",
        "$$z =\\sum \\limits _{i=1} ^{m} w_{i}x_{i} + bias $$\n",
        "$$ \\sigma(z)=\\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "\n",
        "$w_i$ represents the learnable parameters(theta) and bias represent $\\theta_0$. Sigmoid functions helps to limit the ouput between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hypothesis(z) and sigmoid function together\n",
        "\n",
        "\n",
        "def sigmoid(z):        \n",
        "    return 1 / (1 + np.exp(-z))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. predict\n",
        "\n",
        "Predict an output value for a given x with a set of coefficients. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# get the sigmoid vlaue and categorize classes 0 and 1\n",
        "\n",
        "def predict(lr_theta,feature_vector_normalized):\n",
        "    z = feature_vector_normalized.dot(lr_theta)\n",
        "    predicted_y_sigmoid = sigmoid(z)\n",
        "    predicted_y = (predicted_y_sigmoid > 0.5).astype(int)\n",
        "    # predicted_y = np.array([1 if value>0.5 else 0 for value in predicted_y_sigmoid ])\n",
        "    # predicted_y = predicted_y.reshape(-1,1)\n",
        "    return predicted_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(80, 1)\n",
            "(80, 1)\n"
          ]
        }
      ],
      "source": [
        "# random theta/weights intialization\n",
        "\n",
        "n = train_set_x_normalized.shape[1]\n",
        "lr_theta = np.zeros(n)\n",
        "# lr_theta = np.random.randn(n,1)\n",
        "lr_theta = lr_theta.reshape(-1,1)\n",
        "# print('Theta values:')\n",
        "# print(lr_theta)\n",
        "\n",
        "predicted_y = predict(lr_theta,train_set_x_normalized)\n",
        "print(predicted_y.shape)\n",
        "print(train_set_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. accurate\n",
        "\n",
        "Calculate accuracy percentage of the predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4625\n"
          ]
        }
      ],
      "source": [
        "def accuracy(actual_y, predicted_y):\n",
        "    comparision = actual_y ==predicted_y\n",
        "    accuracy = comparision.mean()\n",
        "    return accuracy\n",
        "\n",
        "accuracy_score = accuracy(train_set_y,predicted_y)\n",
        "print(accuracy_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. coef_gd\n",
        "\n",
        "Estimate logistic regression coefficients using **vanilla gradient descent**. Using **the cross entropy loss**. Carefully choose learning rate and epochs values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For zero theta initialization:\n",
            "Accuracy -> 46.25%\n",
            "Loss -> 0.6931471785599453\n"
          ]
        }
      ],
      "source": [
        "def cross_entropy_loss(train_x,train_y,lr_theta):\n",
        "    \n",
        "    z = train_x.dot(lr_theta)\n",
        "\n",
        "    predicted_y = sigmoid(z)\n",
        "\n",
        "    class_zero_loss = train_y * np.log(predicted_y+1e-9)\n",
        "    class_one_loss = (1-train_y) * np.log(1 - predicted_y+1e-9)\n",
        "    loss =  - np.mean(class_zero_loss + class_one_loss)\n",
        "    return loss     \n",
        "    \n",
        "\n",
        "loss = cross_entropy_loss(train_set_x_normalized,train_set_y,lr_theta)\n",
        "\n",
        "print('For zero theta initialization:')\n",
        "print(f'Accuracy -> {accuracy_score*100}%')\n",
        "print(f'Loss -> {loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given:\n",
            "epoch--> 800\n",
            "alpha--> 0.1 \n",
            "\n",
            "Final values:\n",
            "Cost--> 0.02347434225366581, 0.028910924583946413\n",
            "Accuracy--> 1.0, 1.0\n"
          ]
        }
      ],
      "source": [
        "# Vanilla gradient descent to find the optimal fit\n",
        "\n",
        "def vanilla_gradient_descent(epoch,alpha,train_set_x_normalized,train_set_y,val_set_x_biased,val_set_y,lr_theta):\n",
        "    iterator = 0\n",
        "    lr_train_cost_history = [] \n",
        "    lr_val_cost_history = []\n",
        "    lr_train_accuracy_history = [] \n",
        "    lr_val_accuracy_history = []\n",
        "\n",
        "    m = train_set_x_normalized.shape[0]   \n",
        "\n",
        "    while iterator < epoch:\n",
        "        z = train_set_x_normalized.dot(lr_theta)\n",
        "\n",
        "        predicted_y_sigmoid = sigmoid(z)\n",
        "        diff = predicted_y_sigmoid-train_set_y     \n",
        "        update = train_set_x_normalized.T.dot(diff)        \n",
        "        lr_theta -= (alpha/m) * update\n",
        "        \n",
        "        lr_train_cost_history.append(cross_entropy_loss(train_set_x_normalized,train_set_y,lr_theta))\n",
        "        lr_val_cost_history.append(cross_entropy_loss(val_set_x_biased,val_set_y,lr_theta))\n",
        "\n",
        "        train_predicted_y = predict(lr_theta,train_set_x_normalized)\n",
        "        lr_train_accuracy_history.append(accuracy(train_set_y,train_predicted_y))\n",
        "\n",
        "        val_predicted_y = predict(lr_theta,val_set_x_biased)\n",
        "        lr_val_accuracy_history.append(accuracy(val_set_y,val_predicted_y))\n",
        "\n",
        "        iterator += 1\n",
        "    return lr_theta,lr_train_cost_history,lr_val_cost_history,lr_train_accuracy_history,lr_val_accuracy_history\n",
        "\n",
        "\n",
        "\n",
        "epoch = 800\n",
        "alpha = 0.1\n",
        "\n",
        "lr_theta = np.zeros(train_set_x_normalized.shape[1])\n",
        "lr_theta = lr_theta.reshape(-1,1)\n",
        "\n",
        "val_set_x_biased = np.concatenate((np.ones((val_set_y.shape[0],1)),val_set_x),axis=1)\n",
        "val_set_x_normalized = normalize(val_set_x_biased)\n",
        "lr_theta,lr_train_cost_history,lr_val_cost_history ,lr_train_accuracy_history,lr_val_accuracy_history= vanilla_gradient_descent(epoch,alpha,train_set_x_normalized,train_set_y,val_set_x_normalized, val_set_y, lr_theta)\n",
        "\n",
        "print(f'Given:\\nepoch--> {epoch}\\nalpha--> {alpha} ')\n",
        "print(f'\\nFinal values:\\nCost--> {lr_train_cost_history[-1]}, {lr_val_cost_history[-1]}\\nAccuracy--> {lr_train_accuracy_history[-1]}, {lr_val_accuracy_history[-1]}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. draw_model\n",
        "\n",
        "a) Plot the both training loss and validation loss for each epochs.\n",
        "\n",
        "b) Plot the both training accuracy and validation accuracy for each epochs. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(len(lr_train_cost_history)), lr_train_cost_history, label='Train Loss')\n",
        "plt.plot(range(len(lr_val_cost_history)), lr_val_cost_history, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(lr_train_accuracy_history)), lr_train_accuracy_history, label='Train Acuuracy')\n",
        "plt.plot(range(len(lr_val_accuracy_history)), lr_val_accuracy_history, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8. predict the testing data\n",
        "\n",
        "Use your pre-trained model to predict the testing data. Print out your **testing accurate**. Is it good? If not, analyze the reason in short and modify your code to improve.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "test_x_biased = bias(test_x)\n",
        "test_x_normalized = normalize(test_x_biased)\n",
        "test_predicted_y = predict(lr_theta,test_x_normalized)\n",
        "test_y = test_y.reshape(-1,1)\n",
        "print(accuracy(test_y,test_predicted_y))\n",
        "#    print(test_x_normalized.shape, test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The testing accuracy for the model is good. It varies depending on the epoch and learning rate we set in the model. Tried multiple values of learning rate(alpha) and epochs and identified the best values on comparison for training and testing accuracy. Based on the comparison,the learning rate is set  to make the optimisation procedure more stable and effective, and epoch number to make sure the model will converge therby having reasonable trraining and testing cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eNsYoFL7pOG"
      },
      "source": [
        "### - Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTuXPz9V8jIA"
      },
      "source": [
        "In this part, you are going to implement your own naive bayes classifier. \n",
        "You need to implement naive bayes classifier from scratch **using the training data of wine datasets train_x and train_y**. The required functions are listed below. You can add more functions as you need. **No library versions of naive bayes classifier are allowed**. \n",
        "_________\n",
        "\n",
        "1. train_val_split\n",
        "\n",
        "**Randomly** split training data into train and val set. 80% of the training data will be the train set and 20% of the training data will be the val set.\n",
        "\n",
        "2. normalization (data preprocessing)\n",
        "\n",
        "You should normalize all data for each attribute firstly. \n",
        "\n",
        "3. cross_validation_split\n",
        "\n",
        "**Randomly** split data into 5 folds.\n",
        "\n",
        "\n",
        "4. predict\n",
        "\n",
        "Predict the class label for a given x. \n",
        "\n",
        "5. accurate\n",
        "\n",
        "Calculate accuracy percentage of the predictions. Remember to average the results of k folds\n",
        "\n",
        "6. gaussian_probability\n",
        "\n",
        "Calculate the Gaussian probability distribution function for the given x. \n",
        "\n",
        "![Image_20210920015911.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "5. class_probability\n",
        "\n",
        "Calculate the probabilities of predicting each class for a given x using naive bayes algorithm. Be aware that we have multiple input variables. And you may want to use gaussian_probability function here.\n",
        "\n",
        "*Print out your accuracy result. Is it good? If not, analyze the reason in short. *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. train_val_split\n",
        "\n",
        "**Randomly** split training data into train and val set. 80% of the training data will be the train set and 20% of the training data will be the val set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f-jO9XrADS92"
      },
      "outputs": [],
      "source": [
        "def nb_train_val_split(train_x,train_y):\n",
        "\n",
        "   from sklearn.model_selection import train_test_split\n",
        "   nb_train_x, nb_val_x, nb_train_y, nb_val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=10)\n",
        "   nb_train_y = nb_train_y.reshape(-1,1)\n",
        "   nb_val_y = nb_val_y.reshape(-1,1)\n",
        "\n",
        "   return nb_train_x, nb_val_x, nb_train_y, nb_val_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. normalization (data preprocessing)\n",
        "\n",
        "You should normalize all data for each attribute firstly. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "    \n",
        "def normalize(feature_vector):\n",
        "   \n",
        "\n",
        "   mean = np.mean(feature_vector, axis=0)\n",
        "   max = np.amax(feature_vector, axis=0)\n",
        "   min = np.amin(feature_vector, axis=0)\n",
        "   normalized_feature_vector=(feature_vector-mean)/(max-min)\n",
        "   \n",
        "   # mean = np.mean(feature_vector,axis=0)\n",
        "   # std = np.std(feature_vector,axis=0)    \n",
        "   # normalized_feature_vector = (feature_vector - mean ) / (std)\n",
        "    \n",
        "   return normalized_feature_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. cross_validation_split\n",
        "\n",
        "**Randomly** split data into 5 folds.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_validation_split(train_x,train_y,num_folds):\n",
        "   \n",
        "    total = len(train_y)\n",
        " \n",
        "    fold_size = total // num_folds\n",
        "    train = np.column_stack((train_x,train_y))\n",
        "  \n",
        "    np.random.shuffle(train)\n",
        "    folds = []\n",
        "\n",
        "    for i in range (num_folds):\n",
        "        start = i*fold_size\n",
        "        end = start+fold_size\n",
        "        fold = train[start:end]\n",
        "        \n",
        "        fold_x = fold[:, :-1]\n",
        "        fold_y = fold[:, -1].astype(int).reshape(-1,1)\n",
        "        \n",
        "\n",
        "        folds.append((fold_x,fold_y))\n",
        "\n",
        "    return folds    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_val_set_creation(folds,val_fold_index):\n",
        "    \n",
        "    nb_train_set_x = []\n",
        "    nb_train_set_y = []\n",
        "\n",
        "    for i,fold in enumerate(folds):\n",
        "        if i == val_fold_index:\n",
        "            nb_val_set_x, nb_val_set_y = fold\n",
        "        else:\n",
        "            nb_train_set_x.append(fold[0])\n",
        "            nb_train_set_y.append(fold[1])\n",
        "    \n",
        "\n",
        "    nb_train_set_x = np.concatenate(nb_train_set_x)\n",
        "    nb_train_set_y = np.concatenate(nb_train_set_y)\n",
        "    \n",
        "    # nb_val_set_y = nb_val_set_y.reshape(-1,1)\n",
        "    # nb_train_set_y = nb_train_set_y.reshape(-1,1)\n",
        "\n",
        "    return nb_train_set_x,nb_train_set_y,nb_val_set_x,nb_val_set_y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "4. predict\n",
        "\n",
        "Predict the class label for a given x. \n",
        "\n",
        "5. accurate\n",
        "\n",
        "Calculate accuracy percentage of the predictions. Remember to average the results of k folds\n",
        "\n",
        "6. gaussian_probability\n",
        "Calculate the Gaussian probability distribution function for the given x. \n",
        "\n",
        "5. class_probability\n",
        "\n",
        "Calculate the probabilities of predicting each class for a given x using naive bayes algorithm. Be aware that we have multiple input variables. And you may want to use gaussian_probability function here.\n",
        "\n",
        "*Print out your accuracy result. Is it good? If not, analyze the reason in short. *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prior_prob_fn(train_y):\n",
        "    classes, count = np.unique(train_y, return_counts=True)\n",
        "    num_classes = len(classes)\n",
        "    prior_prob = np.zeros(num_classes)\n",
        "    for index in range(num_classes):        \n",
        "        prior_prob[index] = count[index] / train_y.shape[0]    \n",
        "    return np.log(prior_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mu_sigma(train_x,train_y):\n",
        "    \n",
        "    classes = np.unique(train_y)\n",
        "    num_classes = len(classes)\n",
        "    num_features = train_x.shape[1]\n",
        "    mu = np.zeros((num_classes,num_features))\n",
        "    sigma = np.zeros((num_classes,num_features))\n",
        "\n",
        "    for clas in classes:\n",
        "            class_features = train_x[np.where(train_y==clas)]\n",
        "            mu[clas, :] = np.mean(class_features,axis=0)\n",
        "            sigma[clas, :] = np.std(class_features,axis=0)\n",
        "               \n",
        "    return mu, sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gaussian_prob(X,mu,sigma): \n",
        "    cls_cond_prob = []   \n",
        "    \n",
        "    for index in range(mu.shape[0]):\n",
        "        numerator = np.exp(-(1/2)*((X - mu[index])/sigma[index])**2)\n",
        "        denominator = sigma[index] * np.sqrt(2*np.pi)\n",
        "        prob = (numerator/denominator)\n",
        "        \n",
        "        cls_cond_prob.append(prob)\n",
        "        \n",
        "\n",
        "    return cls_cond_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def class_prob(Y,prior_prob,cls_cond_prob):\n",
        "    classes = np.unique(Y)\n",
        "    posterior_prob = []\n",
        "    for index in range(len(classes)):\n",
        "            prior = prior_prob[index]\n",
        "            posterior = np.sum(np.log(cls_cond_prob[index]),axis=1)\n",
        "            posterior = prior + posterior            \n",
        "            posterior_prob.append(posterior)\n",
        "\n",
        "    posterior_prob = np.array(posterior_prob).T\n",
        "\n",
        "    return posterior_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nb_predict(posterior_prob):\n",
        "    \n",
        "    return np.argmax(posterior_prob, axis = 1).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nb_accuracy(actual_y, nb_predicted_y):\n",
        "    accuracy = np.sum(actual_y == nb_predicted_y) / len(actual_y)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average fold accuracy -> 0.86\n"
          ]
        }
      ],
      "source": [
        "# Reshape, bias, normalize the original x and y sets\n",
        "\n",
        "train_y = train_y.reshape(-1,1)\n",
        "test_y = test_y.reshape(-1,1)\n",
        "\n",
        "train_x = normalize(train_x)\n",
        "\n",
        "test_x = normalize(test_x)\n",
        "\n",
        "\n",
        "num_folds = 5\n",
        "accuracy = []\n",
        "\n",
        "folds = cross_validation_split(train_x,train_y,num_folds)\n",
        "\n",
        "for val_fold_index in range(num_folds):\n",
        "\n",
        "    nb_train_set_x,nb_train_set_y,nb_val_set_x,nb_val_set_y = train_val_set_creation(folds,val_fold_index)\n",
        "\n",
        "    prior_prob = prior_prob_fn(nb_train_set_y)  \n",
        "    mu,sigma = mu_sigma(nb_train_set_x,nb_train_set_y)\n",
        "\n",
        "    cls_cond_prob = gaussian_prob(nb_val_set_x,mu,sigma)\n",
        "    \n",
        "    posterior_prob = class_prob(nb_val_set_y,prior_prob,cls_cond_prob)\n",
        "    \n",
        "    \n",
        "    nb_predicted_y= nb_predict(posterior_prob)\n",
        "    fold_accuracy = nb_accuracy(nb_val_set_y,nb_predicted_y)\n",
        "    accuracy.append(fold_accuracy)\n",
        "\n",
        "    final_accuracy = np.mean(accuracy)\n",
        "\n",
        "print(f'Average fold accuracy -> {final_accuracy:.2f}') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy -> 0.93\n"
          ]
        }
      ],
      "source": [
        "prior_prob = prior_prob_fn(train_y)  \n",
        "mu,sigma = mu_sigma(train_x,train_y)\n",
        "\n",
        "cls_cond_prob = gaussian_prob(test_x,mu,sigma)\n",
        "\n",
        "posterior_prob = class_prob(test_y,prior_prob,cls_cond_prob)\n",
        "\n",
        "nb_predicted_y= nb_predict(posterior_prob)\n",
        "\n",
        "test_accuracy = nb_accuracy(test_y,nb_predicted_y)\n",
        "\n",
        "print(f'Test accuracy -> {test_accuracy:.2f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
